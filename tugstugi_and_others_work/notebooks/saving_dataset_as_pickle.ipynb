{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../mongolian_bert_sentencepiece/mn_uncased.model')\n",
    "\n",
    "def sp_tokenize(w):\n",
    "    return sp.EncodeAsPieces(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c500bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Сайн байна уу?', 'Танд энэ өдрийн мэнд хүргье.', 'Монгол текст ангилах гэж байна.']\n",
      "['Монгол', 'улсын', 'их', 'хурал']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "print(nltk.sent_tokenize(\"Сайн байна уу? Танд энэ өдрийн мэнд хүргье. Монгол текст ангилах гэж байна.\"))\n",
    "print(nltk.word_tokenize(\"Монгол улсын их хурал\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68d0e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80036, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = '../../main_dataset/'\n",
    "trained_model_and_fitted_encoder_path = '../models/'\n",
    "plots_path = '../plots'\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1OX-s4n6Go_RsTPLlO48IclpCPqcXkkfo/view?usp=sharing'\n",
    "path = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe9cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"B абвгдеёжзийклмноөпрстуүфхцчшъыьэюя\"  # B: blank\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def convert_text(text):\n",
    "    text = text.lower()\n",
    "    # ignore all characters which is not in the vocabulary\n",
    "    return [char2idx[char] for char in text if char != 'B' and char in char2idx]\n",
    "\n",
    "\n",
    "new_text = [convert_text(text) for text in df['content']]\n",
    "# Checking if content is written in latin letters\n",
    "converted_text_sum = [i for i in range(len(new_text)) if sum(new_text[i]) == len(new_text[i])]\n",
    "\n",
    "# Dropping contents written in latin letters\n",
    "df = df.drop(df.index[converted_text_sum])\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c06ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stopwordsmn = ['аа','аанхаа','алив','ба','байдаг','байжээ','байна','байсаар','байсан',\n",
    "               'байхаа','бас','бишүү','бол','болжээ','болно','болоо','бэ','вэ','гэж','гэжээ',\n",
    "               'гэлтгүй','гэсэн','гэтэл','за','л','мөн','нь','тэр','уу','харин','хэн','ч',\n",
    "               'энэ','ээ','юм','үү','?','', '.', ',', '-','ийн','ын','тай','г','ийг','д','н',\n",
    "               'ний','дээр','юу']\n",
    "\n",
    "df_preprocessed           = []\n",
    "df_preprocessed_stopwords = []\n",
    "word_dict   = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    news  = row['content']\n",
    "    label = row['type_text']\n",
    "    sentences = nltk.sent_tokenize(news)\n",
    "    content_sentences           = []\n",
    "    content_sentences_stopwords = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens   = nltk.word_tokenize(sentence)\n",
    "        tokens   = [w.lower() for w in tokens]\n",
    "        table    = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words    = [word for word in stripped if word.isalpha()]\n",
    "        words_stopwords = [w for w in words if not w in stopwordsmn]\n",
    "        \n",
    "        content_sentences.append(words)\n",
    "        content_sentences_stopwords.append(words_stopwords)\n",
    "        \n",
    "        for w in words:\n",
    "            word_dict[w] = 0\n",
    "            \n",
    "    df_preprocessed.append([content_sentences, label])\n",
    "    df_preprocessed_stopwords.append([content_sentences_stopwords, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16aebcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to 1111_dataset.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../dataset/1111_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_preprocessed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to 1111_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c179e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to 1111_stopwords_removed.pickle\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/1111_stopwords_removed.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_preprocessed_stopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to 1111_stopwords_removed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fb342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "word_index[\"<PAD>\"   ] = 0\n",
    "word_index[\"<START>\" ] = 1\n",
    "word_index[\"<UNK>\"   ] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "cnt = 4\n",
    "for k, v in word_dict.items():\n",
    "    word_index[k] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "#print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30cab627",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d347481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to word_index.pickle\n",
      "saved to reversed_word_index.pickle\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to word_index.pickle\")\n",
    "    \n",
    "with open('../dataset/reversed_word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(reversed_word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to reversed_word_index.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
