{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../mongolian_bert_sentencepiece/mn_uncased.model')\n",
    "\n",
    "def sp_tokenize(w):\n",
    "    return sp.EncodeAsPieces(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c500bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Сайн байна уу?', 'Танд энэ өдрийн мэнд хүргье.', 'Монгол текст ангилах гэж байна.']\n",
      "['Монгол', 'улсын', 'их', 'хурал']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "print(nltk.sent_tokenize(\"Сайн байна уу? Танд энэ өдрийн мэнд хүргье. Монгол текст ангилах гэж байна.\"))\n",
    "print(nltk.word_tokenize(\"Монгол улсын их хурал\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68d0e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source_text</th>\n",
       "      <th>status_text</th>\n",
       "      <th>type_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Нийслэлийн Засаг даргын Тамгын газар</td>\n",
       "      <td>Дулааны тухай</td>\n",
       "      <td>2012-10-13T16:33:29.484371Z</td>\n",
       "      <td>Дуудлага</td>\n",
       "      <td>Хаагдсан</td>\n",
       "      <td>Гомдол</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ерөнхий сайд</td>\n",
       "      <td>Ерөнхий сайдтай уулзах. Жолоочийн эрх ашиг алд...</td>\n",
       "      <td>2012-10-13T16:33:29.492197Z</td>\n",
       "      <td>Биечлэн</td>\n",
       "      <td>Хаагдсан</td>\n",
       "      <td>Санал хүсэлт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Хүнс, хөдөө аж ахуй, хөнгөн үйлдвэрийн яам</td>\n",
       "      <td>Нефтийн үйлдвэр байгуулах тухай.</td>\n",
       "      <td>2012-10-13T16:33:29.495729Z</td>\n",
       "      <td>Дуудлага</td>\n",
       "      <td>Хаагдсан</td>\n",
       "      <td>Санал хүсэлт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Цагдаагийн ерөнхий газар</td>\n",
       "      <td>Жолооны үнэмлэх яагаад хэвлэгдэхгүй байна ?</td>\n",
       "      <td>2012-10-13T16:33:29.499487Z</td>\n",
       "      <td>Дуудлага</td>\n",
       "      <td>Хаагдсан</td>\n",
       "      <td>Санал хүсэлт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Эрүүл мэндийн яам</td>\n",
       "      <td>БГД Гэмтэл-н эмнэлгийн гадуурх хашааг нураах т...</td>\n",
       "      <td>2012-10-13T16:33:29.503063Z</td>\n",
       "      <td>Дуудлага</td>\n",
       "      <td>Хаагдсан</td>\n",
       "      <td>Гомдол</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       agency  \\\n",
       "0        Нийслэлийн Засаг даргын Тамгын газар   \n",
       "1                                Ерөнхий сайд   \n",
       "2  Хүнс, хөдөө аж ахуй, хөнгөн үйлдвэрийн яам   \n",
       "3                    Цагдаагийн ерөнхий газар   \n",
       "4                           Эрүүл мэндийн яам   \n",
       "\n",
       "                                             content  \\\n",
       "0                                      Дулааны тухай   \n",
       "1  Ерөнхий сайдтай уулзах. Жолоочийн эрх ашиг алд...   \n",
       "2                   Нефтийн үйлдвэр байгуулах тухай.   \n",
       "3        Жолооны үнэмлэх яагаад хэвлэгдэхгүй байна ?   \n",
       "4  БГД Гэмтэл-н эмнэлгийн гадуурх хашааг нураах т...   \n",
       "\n",
       "                    created_at source_text status_text     type_text  \n",
       "0  2012-10-13T16:33:29.484371Z    Дуудлага    Хаагдсан        Гомдол  \n",
       "1  2012-10-13T16:33:29.492197Z     Биечлэн    Хаагдсан  Санал хүсэлт  \n",
       "2  2012-10-13T16:33:29.495729Z    Дуудлага    Хаагдсан  Санал хүсэлт  \n",
       "3  2012-10-13T16:33:29.499487Z    Дуудлага    Хаагдсан  Санал хүсэлт  \n",
       "4  2012-10-13T16:33:29.503063Z    Дуудлага    Хаагдсан        Гомдол  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = '../../main_dataset/'\n",
    "trained_model_and_fitted_encoder_path = '../models/'\n",
    "plots_path = '../plots'\n",
    "\n",
    "df = pd.read_csv(df_path + '1111_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe9cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"B абвгдеёжзийклмноөпрстуүфхцчшъыьэюя\"  # B: blank\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def convert_text(text):\n",
    "    text = text.lower()\n",
    "    # ignore all characters which is not in the vocabulary\n",
    "    return [char2idx[char] for char in text if char != 'B' and char in char2idx]\n",
    "\n",
    "\n",
    "new_text = [convert_text(text) for text in df['content']]\n",
    "# Checking if content is written in latin letters\n",
    "converted_text_sum = [i for i in range(len(new_text)) if sum(new_text[i]) == len(new_text[i])]\n",
    "\n",
    "# Dropping contents written in latin letters\n",
    "df = df.drop(df.index[converted_text_sum])\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c06ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stopwordsmn = ['аа','аанхаа','алив','ба','байдаг','байжээ','байна','байсаар','байсан',\n",
    "               'байхаа','бас','бишүү','бол','болжээ','болно','болоо','бэ','вэ','гэж','гэжээ',\n",
    "               'гэлтгүй','гэсэн','гэтэл','за','л','мөн','нь','тэр','уу','харин','хэн','ч',\n",
    "               'энэ','ээ','юм','үү','?','', '.', ',', '-','ийн','ын','тай','г','ийг','д','н',\n",
    "               'ний','дээр','юу']\n",
    "\n",
    "df_preprocessed           = []\n",
    "df_preprocessed_stopwords = []\n",
    "word_dict   = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    news  = row['content']\n",
    "    label = row['type_text']\n",
    "    sentences = nltk.sent_tokenize(news)\n",
    "    content_sentences           = []\n",
    "    content_sentences_stopwords = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens   = nltk.word_tokenize(sentence)\n",
    "        tokens   = [w.lower() for w in tokens]\n",
    "        table    = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words    = [word for word in stripped if word.isalpha()]\n",
    "        words_stopwords = [w for w in words if not w in stopwordsmn]\n",
    "        \n",
    "        content_sentences.append(words)\n",
    "        content_sentences_stopwords.append(words_stopwords)\n",
    "        \n",
    "        for w in words:\n",
    "            word_dict[w] = 0\n",
    "            \n",
    "    df_preprocessed.append([content_sentences, label])\n",
    "    df_preprocessed_stopwords.append([content_sentences_stopwords, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16aebcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to 1111_dataset.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../dataset/1111_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_preprocessed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to 1111_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c179e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to 1111_stopwords_removed.pickle\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/1111_stopwords_removed.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_preprocessed_stopwords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to 1111_stopwords_removed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fb342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "word_index[\"<PAD>\"   ] = 0\n",
    "word_index[\"<START>\" ] = 1\n",
    "word_index[\"<UNK>\"   ] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "cnt = 4\n",
    "for k, v in word_dict.items():\n",
    "    word_index[k] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "#print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30cab627",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d347481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to word_index.pickle\n",
      "saved to reversed_word_index.pickle\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to word_index.pickle\")\n",
    "    \n",
    "with open('../dataset/reversed_word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(reversed_word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"saved to reversed_word_index.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
