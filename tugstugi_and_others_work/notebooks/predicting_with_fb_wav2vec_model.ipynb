{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4990718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25076be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "df_path = '../../main_dataset/'\n",
    "trained_model_and_fitted_encoder_path = '../models/'\n",
    "plots_path = '../plots'\n",
    "\n",
    "# Loading model\n",
    "reconstructed_model = keras.models.load_model(trained_model_and_fitted_encoder_path + \\\n",
    "                                              'word2vec_fb_pretrained_model.h5')\n",
    "\n",
    "# Loading fitted encoder classes\n",
    "encoder = LabelBinarizer()\n",
    "encoder.classes_ = np.load(trained_model_and_fitted_encoder_path + 'type_texts.npy')\n",
    "\n",
    "# Loading word index\n",
    "with open('../dataset/word_index.pickle', 'rb') as handle:\n",
    "    word_index = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d50ec",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cffc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts to classify intent\n",
    "to_pred = ['Эмийн найдвартай байдлыг хангаж өгнө үү . Эм чимээгүй дайн үүсгэж байна.Ямарч баталгаа алга жнь: хугацаа нь дууссан гэх мэт',\n",
    "           'Сайн байна уу? Та бүхэндээ баярлалаа. Миний санал: өмнөх жилийн удаан буудайн борлуулалтын үнэ 350000 төгрөг байсан. Энэ жил яагаад бууруулахгүй гэчихээд бага үнээр авав?',\n",
    "           'Засгийн газраас хэрэглүүлж байгаа 100000 орон сууцны хөрөнгөө төрийн банкинд өгөөч ээ байрандаа ормоор байна',\n",
    "           'Энэ засгийн газар хүн бүрд сая төгрөг өгөх болох уу?',\n",
    "           'Ордноос шагнал авч байгаа хүмүүс их байна. Засгийн газрын баруун талд машин ачаад явж байна. Жилдээ нэг төрийн шагнал авч байгаа иргэдийг ингэж болдог юм уу.',\n",
    "           'Эрүүл мэндийн даатгалаар Хөнгөлөлттэй эмийг эмийн сангууд сар бүрийн хэдний өдрөөс хэдний өдөр хүртэл олгодог талаар мэдээлэл өгнө үү? Мөн өрхийн эмнэлгүүд хэдэнд эмийн жор бичиж өгдөг вэ? Хөнгөлөлттэй эмийн талаар тодорхой мэдээлэл өгнө үү.',\n",
    "           'БЗД ийн р хороо хогоо ачихгүй сар болж байна. Энэ тал дээр анхаарна уу.',\n",
    "           'ХУД ийн р хорооны нийгмийн ажилтанд . р сард одонгийн материал бүрдүүлэх гэж хандтал хүүхэд ой хүрээгүй мөн материал бүрдүүлэлт авах болоогүй хүүхэд ой хүрсэн хойно авна гэсэн тайлбар хэлж буцаасан. .. нд Хүүхэд ой хүрсэн тул дахин асуудлаар хандтал материал бүрдүүлэлт дууссан шалтгаанаар хүлээж авсангүй. Уг асуудлаар холбогдох бүхий л шатны байгууллагад хандтал материал бүрдүүлэлт дууссан тул ямар ч боломжгүй гэж байна. Уг асуудлыг шийдвэрлэж иргэн намайг хохиролгүй болгож одонгийн материалыг минь хүлээж авч шийдвэрлэнэ үү.',\n",
    "           'СБД ийн р хороо р байрны урд сагсны талбай дээр р сургууль биеийн тамирын хичээл орж шуугиан их гаргаж байна. Үүнээс болж оршин суугчдын амгалан тайван байдал алдагдуулж байна. Мөн сургуулийн сурагчид орцон дотор тамхи татаж байна. Арга хэмжээ авч өгнө үү.',\n",
    "           'Хөвсгөл аймгийн Түнэл сумаас: Сэтгэцийн эрүүл мэндийн р тасагт хоног хэвтэж хүүхдээ эмчлүүлээд гарсан. Картан дээр нь ямар эмчилгээ хийлгэсэн талаар бичсэнгүй. Мөн ямарч зөвөлгөө өгсөнгүй. Эмчилгээ хийсэн эмч нь Насанжаргал. Одоо холбогдох гэхээр утсаа авахгүй байна. Ямар эмчилгээ хийлгэсэн болон одоо яах талаараа зөвөлгөө авахыг хүсч байна.'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317afb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../mongolian_bert_sentencepiece/mn_uncased.model')\n",
    "\n",
    "def sp_tokenize(w):\n",
    "    return sp.EncodeAsPieces(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafdd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "stopwordsmn = ['аа','аанхаа','алив','ба','байдаг','байжээ','байна','байсаар','байсан',\n",
    "               'байхаа','бас','бишүү','бол','болжээ','болно','болоо','бэ','вэ','гэж','гэжээ',\n",
    "               'гэлтгүй','гэсэн','гэтэл','за','л','мөн','нь','тэр','уу','харин','хэн','ч',\n",
    "               'энэ','ээ','юм','үү','?','', '.', ',', '-','ийн','ын','тай','г','ийг','д','н',\n",
    "               'ний','дээр','юу']\n",
    "\n",
    "to_pred_preprocessed = []\n",
    "\n",
    "for entry in to_pred:\n",
    "    sentences = nltk.sent_tokenize(entry)\n",
    "    content_sentences_stopwords = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        words_stopwords = [w for w in words if not w in stopwordsmn]\n",
    "        \n",
    "        content_sentences_stopwords.append(words_stopwords)\n",
    "            \n",
    "    to_pred_preprocessed.append(content_sentences_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285e2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "def encode_content(text):\n",
    "    return [word_index.get(i, 2) for i in text]\n",
    "\n",
    "for i in range(len(to_pred_preprocessed)):\n",
    "    to_pred_preprocessed[i] = list(itertools.chain(*to_pred_preprocessed[i]))[:MAX_LEN]\n",
    "\n",
    "\n",
    "to_pred_entry = [encode_content(sent) for sent in to_pred_preprocessed]\n",
    "to_pred_entry = keras.preprocessing.sequence.pad_sequences(to_pred_entry,\n",
    "                                                           value = word_index[\"<PAD>\"],\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d879a2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text words: ['эмийн', 'найдвартай', 'байдлыг', 'хангаж', 'өгнө', 'эм', 'чимээгүй', 'дайн', 'үүсгэж', 'байнаямарч', 'баталгаа', 'алга', 'жнь', 'хугацаа', 'дууссан', 'гэх', 'мэт'] \n",
      "\n",
      "Encoded shape: (512,) \n",
      "\n",
      "Words encoded and padded: [  274  4778   582   389  8841  4778  1018  1960  4808  5298 53615   486\n",
      "   511  1015  4808 11691   541  4225   486   511  3809    63  1363  5901\n",
      "  2981 20069  1699   486   511  3809   539   837   925 53615  4808 11691\n",
      "   140  7790   325  4362   363   925   926  2567  3351  1538 53615  4808\n",
      " 11691   140   539   317  4303   363   189   641   105  3456 21310  1021\n",
      "  1960  5218  2168   325   295  8437     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print('Text words:', to_pred_preprocessed[0], '\\n')\n",
    "print('Encoded shape:', to_pred_entry[7].shape, '\\n')\n",
    "print('Words encoded and padded:', to_pred_entry[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87b3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text_0: эмийн найдвартай байдлыг хангаж өгнө эм чимээгүй дайн үүсгэж байнаямарч баталгаа алга жнь хугацаа дууссан гэх мэт\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_1: сайн та бүхэндээ баярлалаа миний санал өмнөх жилийн удаан буудайн борлуулалтын үнэ төгрөг жил яагаад бууруулахгүй гэчихээд бага үнээр авав\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_2: засгийн газраас хэрэглүүлж байгаа орон сууцны хөрөнгөө төрийн банкинд өгөөч байрандаа ормоор\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_3: засгийн газар хүн бүрд сая төгрөг өгөх болох\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_4: ордноос шагнал авч байгаа хүмүүс их засгийн газрын баруун талд машин ачаад явж жилдээ нэг төрийн шагнал авч байгаа иргэдийг ингэж болдог\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_5: эрүүл мэндийн даатгалаар хөнгөлөлттэй эмийг эмийн сангууд сар бүрийн хэдний өдрөөс хэдний өдөр хүртэл олгодог талаар мэдээлэл өгнө өрхийн эмнэлгүүд хэдэнд эмийн жор бичиж өгдөг хөнгөлөлттэй эмийн талаар тодорхой мэдээлэл өгнө\n",
      "Predicted type: Талархал \n",
      "\n",
      "Text_6: бзд р хороо хогоо ачихгүй сар болж тал анхаарна\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_7: худ р хорооны нийгмийн ажилтанд р сард одонгийн материал бүрдүүлэх хандтал хүүхэд ой хүрээгүй материал бүрдүүлэлт авах болоогүй хүүхэд ой хүрсэн хойно авна тайлбар хэлж буцаасан нд хүүхэд ой хүрсэн тул дахин асуудлаар хандтал материал бүрдүүлэлт дууссан шалтгаанаар хүлээж авсангүй уг асуудлаар холбогдох бүхий шатны байгууллагад хандтал материал бүрдүүлэлт дууссан тул ямар боломжгүй уг асуудлыг шийдвэрлэж иргэн намайг хохиролгүй болгож одонгийн материалыг минь хүлээж авч шийдвэрлэнэ\n",
      "Predicted type: Талархал \n",
      "\n",
      "Text_8: сбд р хороо р байрны урд сагсны талбай р сургууль биеийн тамирын хичээл орж шуугиан их гаргаж үүнээс болж оршин суугчдын амгалан тайван байдал алдагдуулж сургуулийн сурагчид орцон дотор тамхи татаж арга хэмжээ авч өгнө\n",
      "Predicted type: Гомдол \n",
      "\n",
      "Text_9: хөвсгөл аймгийн түнэл сумаас сэтгэцийн эрүүл мэндийн р тасагт хоног хэвтэж хүүхдээ эмчлүүлээд гарсан картан ямар эмчилгээ хийлгэсэн талаар бичсэнгүй ямарч зөвөлгөө өгсөнгүй эмчилгээ хийсэн эмч насанжаргал одоо холбогдох гэхээр утсаа авахгүй ямар эмчилгээ хийлгэсэн болон одоо яах талаараа зөвөлгөө авахыг хүсч\n",
      "Predicted type: Гомдол \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(to_pred_entry)):\n",
    "    data_words = \" \".join(to_pred_preprocessed[i])\n",
    "    data_indexes = to_pred_entry[i]\n",
    "    print(f'Text_{i}:', data_words)\n",
    "\n",
    "    pred = reconstructed_model.predict([[data_indexes]])\n",
    "    max_arr = pred[np.argmax(pred)]\n",
    "    ind = np.where(max_arr == max_arr.max())\n",
    "    print('Predicted type:', encoder.classes_[ind][0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c72bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fdd07ebeccd95346b1a4b972d73cddb1132119859410c21f6d461d55a357066"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
